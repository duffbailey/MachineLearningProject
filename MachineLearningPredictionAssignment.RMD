---
title: "Prediction Assignment - Bicep Curls"
author: "George D Bailey"
date: '2022-07-11'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r initialize, include=FALSE, cache=TRUE}

library(lubridate)
library(caret)
library(dplyr)
library(leaps)
library(randomForest)
library(leaps)
rm(list = ls())
```

# Overview
This paper describes an assignment to use machine learning to create a model that would use Human Activity Recognition data to identify whether a and observation of a simple "Bicep Curl" exercise was either correct or corresponded to one of four common performance errors.  The training and verification data was gathered from 6 male subjects, with little weightlifting experience, who were trained to properly perform a "Bicep Curl" excercise.  In addition, they were also trained to replicate 4 different common mistakes.  They were outfitted with several motion sensors which measured the velocity of the excercise as well as angle of motion relative to X, Y and Z dimensional planes.

## Approach
The following approach was used:
1. Conduct basic data exploration of the traing dataset. 
2. Prepare the test data and divide it into training and verification sets.
3. Make an educated guess at an initial model to use for training.
4. Assess the accuracy of the trained model against the test dataset.
5. If necessary, further tune the model by employing appropriate techniques such as boosting, bagging and principal component analysis
6. Cross validate the model using the verification data set and assess the resulting out of sample accuracy.
7. Use the model to predict outcomes for the 20 test cases. 


## Data Exploration

A training data set was provided for the purpose of training the model.  The test data set was to be used to generate predictions based on the selected model.  Both data sets included 160 columns of which only 52 had data generated by the sensors used for this exercise.  In addition, there were some columns, such as subject name, which had data that was not relevant to the instrumentation and should not be used for training the model. 


## Data Preparation
Data preparation was performed on the training data set as follows:
1. Columns which had no data were removed.
2. Irrelevant columns were excluded.
3. Rows which had missing data in the remaining columns were excluded.

Fifty percent of the training data set was then reserved for training the model, leaving the remaining 50% for verification. 

```{r data_preparation,  cache=TRUE}
library(lubridate)
library(caret)
library(dplyr)
library(leaps)
library(randomForest)
library(leaps)
setwd("~/R_Projects/MachLearningWk4/R")
trainData = read.csv("pml-training.csv", na.strings = c("null", "NA", ""))
# Exclude columns that don't have data in them
trset <- trainData[,colSums(is.na(trainData))==0]
# Exclude rows that have NA's in any column
trset <- trset %>% select(roll_belt:classe)  %>% na.omit()
# Now the dataset only includes the rows that have complete data in all columns

# Mark 50% of the dataset to use for training and 50% for verification
set.seed(4332)
inTrain = createDataPartition(trset$classe, p=.5)[[1]]
training = trset[inTrain,]
verific = trset[-inTrain,]

```
# Initial Model - 
Because the values of the dependent variable, "Classe", are members of a set of 5 discrete variables, it was hypothesized that a tree type model would generate the best predictions. Hence, a Random Forest method would be created from the training data and verified against the verification data.  If the accuracy was sufficient, the model would be used to generage predictions for the test cases.  Otherwise, additional approaches, such as bagging or boosting would be employed to further tune the model.  


```{r random_forest_model, cache=TRUE}
modelTRRF <- randomForest(as.factor(classe)~., data=training,type="response")

predictionTRRF <- predict(modelTRRF,training)
accuracyTRRF <- sum( training$classe == predictionTRRF)  / length(training$classe)


predictionVERF <- predict(modelTRRF,verific)
accuracyVERF <- sum( verific$classe == predictionVERF)  / length(verific$classe)

RFImportance <- importance(modelTRRF)
RFVarImpPlot <- varImpPlot(modelTRRF)

```
# Model Accuracy and Refinement

The Random Forest model predicted the classe variable in the training data with a `r accuracyTRRF` accuracy rate.  The accuracy rate for the verification data set was `r accuracyVERF`.  The Out of Sample error rate was `r 1 - accuracyVERF`

Based on these results, no additional effort was required to refine or retrain the model.

Figure 1: Importance Plot for Random Forest Model

`r RFVarImpPlot`

As shown, the roll_belt variable had the greatest predictive value.  However, all 52 independent variables contributed to the accuracy of the predictions.


# Prediction Versus the Test Set.

```{r testPredictions, cache=TRUE}
setwd("~/R_Projects/MachLearningWk4/R")
testData = read.csv("pml-testing.csv", na.strings = c("null", "NA", ""))

predictionTERF <- predict(modelTRRF,testData)
```


Figure 2: Predicted Values for 20 Test Cases

`r predictionTERF`.


# Discussion of Results
Superficially, the fact that the Random Forest model was able to predict correct and incorrect performance with 100% accuracy appears astounding.  Deeper analysis, however, suggests several reasons why this result could be expected, given the artificial circumstances of a highly controlled experiment.

As outlined in the document describing the experiment, the exercise being evaluated (Bicep Curl) involves a simple angular motion of a hinge joint with minimal movement elsewhere in the body.  It should be expected that correct performance (Classe = A) would elicit a very narrow range of values from the measuring devices used in the experiment. Likewise, it should also be possible for the measuring instruments to clearly differentiate the various "errors" which test subjects were asked to simulate.

In a realistic setting, subjects would be working with heavier weights and there would be a more continuous relationship between correct and incorrect performance. In addition, real weight training subject might be expected to make several errors in the same movement (over engagement of the back, incorrect plane of motion and incomplete flexion) or have attempts that are "almost perfect".  It is conceivable that a linear model might be better suited in these cases.

